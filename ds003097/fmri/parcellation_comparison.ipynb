{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "developed-phenomenon",
   "metadata": {},
   "source": [
    "# How to determine which parcellation is best?\n",
    "\n",
    "The goal of this notebook will be relatively straightforward, essentially we want to ask the question, how does different choice of parcellation influence performance? This example notebook assumes that we have already run the set of pre-processing steps avaliable in prepare_frmi_data.ipynb.\n",
    "\n",
    "- How do you know which steps should be divided into front-end pre-processing and which steps should be nested in cross-validation?\n",
    "\n",
    "As a rule of thumb, you can distinguish between steps that operate on a single individuals data and those that require information across multiple subjects. Essentially, any step or transformation that can be computed and applied on the level of the individual, it is fine to do these steps ahead of time. On the otherhand, any steps, like column-wise standardization / z-score, that involve information as computed across a group of subjects, should be down in a properly nested manner / as nested within a machine learning pipeline.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-columbia",
   "metadata": {},
   "source": [
    "Broader topic is: How to structure and answer a question of interest within a predictive framework?\n",
    "\n",
    "This can be the general structure of the chapter, weaving in this specific example problem.\n",
    "In terms of specifics, let's perform a global train-test split, then on the training set investigate different choices of ML model / connectivity measure, then use the Test split to answer our main topic of interest, the different parcellations.\n",
    "\n",
    "\n",
    "- Broadly introduce what a predictive framework is\n",
    "\n",
    "- Examples on how to frame questions in terms of prediction, e.g., basic, to more clever setups, so that you can answer your question based on the difference in performance from different setups.\n",
    "\n",
    "- Introduce the actual example we will use here + any other relevant background. Mntion the extra-preprocessing / preperation / link the notebook.\n",
    "\n",
    "- The importance of generalizability, w/ example of pitfall, i.e., training accuracy\n",
    "\n",
    "- Note the relativity of  estimating generalizability, or of the generalizability one might care about, as related to common neuro-imaging datasets, e.g., maybe you care about generalizing to new unseen sites or people from a different country, or maybe you don't, and how you can build that into \n",
    "\n",
    "- Techniques to estimate generalizability - Train / Test, K-Fold CV\n",
    "\n",
    "- General structure of an ML predictive pipeline, essentially designed to be used in context where estimating generalizability is desired\n",
    "\n",
    "- Then more specific structure / choices of an ML predictive pipeline for this question / task based connectivity, at the start of this section, \n",
    "\n",
    "- How do you know which steps should be divided into front-end pre-processing and which steps should be nested in cross-validation?\n",
    "\n",
    "- Choice of cross-validation technique as related to main question of interest / pitfalls of CV\n",
    "\n",
    "- Neuroimaging specific things, like confounds, and *briefly* introduce some different ways of 1. Detecting the influence of potential confounds, 2. Trying to correct for their influence. \n",
    "\n",
    "- Feature importances / the difficulty in making sense of feature importances when the underlying data is complex like with task connectivity. Make a just terrible overwhelming visualization of 10k connections on a brain.\n",
    "\n",
    "- Limitations of a predictive framework (correlation not causation, just because one method is not predictive doesn't mean a different method isn't)\n",
    "\n",
    "TODO: Put this piece later on, mixed in which the real examples~\n",
    "\n",
    "## Important conceptual note on the choice of cross-validation technique as related to main question of interest\n",
    "\n",
    "In this notebook our main question of interest is the comparison of performance as broken down by parcellation. This is important because it directly influences our choices in how we setup our cross-validation. Specifically, we will be performing 5-fold cross validation (CV) in this notebook across the entire set of avaliable subjects. This is an okay choice because ultimately every single set of results from every 5-fold CV we run are a part of our main results of interest. This is an important point! \n",
    "\n",
    "It is likely easier to think about this concept in terms of an example on what you are not supposed to do. Let's say instead our question of interest wasn't related to the parcellation, but instead on how well we could predict a certain phenotype, let's say age, from our task connectivity. Then, as a sub question maybe we have the same setup as we do here, 6 different parcellations from a mix of volumetric and surface based avaliable, so we proceed in exactly the same way and try all 6 different parcellations with 5-fold cross validation on the full set of avaliable subjects. Then, we take the one that did best and then move on from there just using that parcellation, and we dig into those results, examine the feature importances, and report just those results, for that parcellation. So why is that wrong? Essentially, because cross-validation isn't fullproof. Consider another example, where we do the same thing, but this time we have 100,000 randomly generated parcellations. We go through and test every single one, and 6 months later when everything is finnally done running, we find one of those parcellations has done amazing! So we say great, and just report those results, even though now its becoming more and more clear that this result was almost certainly a result of dumb luck.\n",
    "\n",
    "The core issue in both cases is that these results would be presented with no gaurentee of generalizability, granted to different degrees. I think for some reason especially when youn are first getting started with machine learning, it is too easy to think of cross-validation techniques, like K-fold, as a silver bullet. When really, these estimates you get out from them are potentially noisy, especially at smaller sample sizes! What this comes down to is keeping generalizability in mind.\n",
    "\n",
    "That is to say, in practice, you must make sure that any \"hyper-parameters\" within the machine learning pipeline (in the example here, choice of parcellation is a hyper-parameter, but these broadly refer to ANY choice which is not directly a part your research question) is not informed by the set of data ultimately used to estimate generalizability (either a test set, or the validation sets in a K-Fold setup). Explicitly for each 'hyper-parameter' you have the following options:\n",
    "\n",
    "- If this parameter is important to your research topic, both test and report the results by each possible value of interest that parameter might take.\n",
    "\n",
    "- Otherwise, if not important / directly related, you have a few options:\n",
    "    1. Fix the value ahead of time based on some aprioi knowledge or guess.\n",
    "    2. Assign the value through nested cross-validation (e.g., train-val-test split or nested K-fold)\n",
    "\n",
    "So for example, if my question of interest included: \"what's better a Random Forest or a Ridge Regression?\", it would be desirable to test both of these estimators directly, but if it wasn't, then we would want to either just fix the choice ahead of time. Let's say we know ridge regression has worked well in the past on simmilar problems, so we can just choose to use it. Or, we could take the tact of assigning it through nested cross validation. Now this second option has plently of different variations. For example, one way would be to split our original training dataset into a further training and validation dataset, then to test each model on the validation set and use that choice to inform our final choice, which we would ultimatly test on the test set. Another variation would be, within the training set, perform say 5-Fold cross validation, and whichever model gets the highest average score, select that one. \n",
    "\n",
    "This concept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-charge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lesser-software",
   "metadata": {},
   "source": [
    "First, we load our prepared BPt dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "indonesian-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import BPt as bp\n",
    "import os\n",
    "from os.path import dirname, abspath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "external-practitioner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"float: left; padding: 10px;\">\n",
       "        <h3>Data</h3><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basc_444</th>\n",
       "      <th>difumo_256</th>\n",
       "      <th>gordon</th>\n",
       "      <th>hcp_mmp</th>\n",
       "      <th>juelich</th>\n",
       "      <th>schaefer_400</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-0001</th>\n",
       "      <td>Loc(4901)</td>\n",
       "      <td>Loc(3139)</td>\n",
       "      <td>Loc(2264)</td>\n",
       "      <td>Loc(522)</td>\n",
       "      <td>Loc(4020)</td>\n",
       "      <td>Loc(1393)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0002</th>\n",
       "      <td>Loc(5143)</td>\n",
       "      <td>Loc(3381)</td>\n",
       "      <td>Loc(2504)</td>\n",
       "      <td>Loc(762)</td>\n",
       "      <td>Loc(4262)</td>\n",
       "      <td>Loc(1633)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0003</th>\n",
       "      <td>Loc(4640)</td>\n",
       "      <td>Loc(2878)</td>\n",
       "      <td>Loc(2005)</td>\n",
       "      <td>Loc(263)</td>\n",
       "      <td>Loc(3759)</td>\n",
       "      <td>Loc(1134)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0004</th>\n",
       "      <td>Loc(5155)</td>\n",
       "      <td>Loc(3393)</td>\n",
       "      <td>Loc(2516)</td>\n",
       "      <td>Loc(774)</td>\n",
       "      <td>Loc(4274)</td>\n",
       "      <td>Loc(1645)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0005</th>\n",
       "      <td>Loc(4668)</td>\n",
       "      <td>Loc(2906)</td>\n",
       "      <td>Loc(2033)</td>\n",
       "      <td>Loc(291)</td>\n",
       "      <td>Loc(3787)</td>\n",
       "      <td>Loc(1162)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0923</th>\n",
       "      <td>Loc(4721)</td>\n",
       "      <td>Loc(2959)</td>\n",
       "      <td>Loc(2086)</td>\n",
       "      <td>Loc(344)</td>\n",
       "      <td>Loc(3840)</td>\n",
       "      <td>Loc(1215)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0924</th>\n",
       "      <td>Loc(5150)</td>\n",
       "      <td>Loc(3388)</td>\n",
       "      <td>Loc(2511)</td>\n",
       "      <td>Loc(769)</td>\n",
       "      <td>Loc(4269)</td>\n",
       "      <td>Loc(1640)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0925</th>\n",
       "      <td>Loc(4794)</td>\n",
       "      <td>Loc(3032)</td>\n",
       "      <td>Loc(2158)</td>\n",
       "      <td>Loc(416)</td>\n",
       "      <td>Loc(3913)</td>\n",
       "      <td>Loc(1287)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0926</th>\n",
       "      <td>Loc(4900)</td>\n",
       "      <td>Loc(3138)</td>\n",
       "      <td>Loc(2263)</td>\n",
       "      <td>Loc(521)</td>\n",
       "      <td>Loc(4019)</td>\n",
       "      <td>Loc(1392)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0927</th>\n",
       "      <td>Loc(4813)</td>\n",
       "      <td>Loc(3051)</td>\n",
       "      <td>Loc(2177)</td>\n",
       "      <td>Loc(435)</td>\n",
       "      <td>Loc(3932)</td>\n",
       "      <td>Loc(1306)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>871 rows × 6 columns</p>\n",
       "</div></div>\n",
       "<div style=\"float: left; padding: 10px;\">\n",
       "        <h3>Targets</h3><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIS</th>\n",
       "      <th>BMI</th>\n",
       "      <th>IST_intelligence_total</th>\n",
       "      <th>NEO_N</th>\n",
       "      <th>STAI_T</th>\n",
       "      <th>age</th>\n",
       "      <th>education_level</th>\n",
       "      <th>religious_now</th>\n",
       "      <th>religious_upbringing</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-0001</th>\n",
       "      <td>16</td>\n",
       "      <td>23.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>28</td>\n",
       "      <td>44.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0002</th>\n",
       "      <td>20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>30</td>\n",
       "      <td>31.0</td>\n",
       "      <td>21.75</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0003</th>\n",
       "      <td>20</td>\n",
       "      <td>31.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>34</td>\n",
       "      <td>40.0</td>\n",
       "      <td>25.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0004</th>\n",
       "      <td>22</td>\n",
       "      <td>20.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>29</td>\n",
       "      <td>32.0</td>\n",
       "      <td>22.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0005</th>\n",
       "      <td>20</td>\n",
       "      <td>23.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>27</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0923</th>\n",
       "      <td>20</td>\n",
       "      <td>19.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>38</td>\n",
       "      <td>53.0</td>\n",
       "      <td>21.75</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0924</th>\n",
       "      <td>22</td>\n",
       "      <td>21.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>40</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0925</th>\n",
       "      <td>13</td>\n",
       "      <td>30.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>28</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0926</th>\n",
       "      <td>20</td>\n",
       "      <td>22.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>27</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0927</th>\n",
       "      <td>23</td>\n",
       "      <td>35.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>35</td>\n",
       "      <td>38.0</td>\n",
       "      <td>24.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>871 rows × 10 columns</p>\n",
       "</div></div>\n"
      ],
      "text/plain": [
       "                 BMI  IST_intelligence_total    age sex education_level  \\\n",
       "participant_id                                                            \n",
       "sub-0001        23.0                   159.0  22.00   0               2   \n",
       "sub-0002        20.0                   199.0  21.75   0               2   \n",
       "sub-0003        31.0                   227.0  25.25   0               0   \n",
       "sub-0004        20.0                   270.0  22.50   0               0   \n",
       "sub-0005        23.0                   212.0  22.25   1               0   \n",
       "...              ...                     ...    ...  ..             ...   \n",
       "sub-0923        19.0                   153.0  21.75   1               2   \n",
       "sub-0924        21.0                   246.0  22.25   1               2   \n",
       "sub-0925        30.0                   150.0  25.25   1               2   \n",
       "sub-0926        22.0                   161.0  20.75   1               0   \n",
       "sub-0927        35.0                   190.0  24.25   0               2   \n",
       "\n",
       "                STAI_T  BIS  NEO_N religious_upbringing religious_now  \\\n",
       "participant_id                                                          \n",
       "sub-0001          44.0   16     28                    0             1   \n",
       "sub-0002          31.0   20     30                    0             0   \n",
       "sub-0003          40.0   20     34                    0             0   \n",
       "sub-0004          32.0   22     29                    1             0   \n",
       "sub-0005          23.0   20     27                    0             0   \n",
       "...                ...  ...    ...                  ...           ...   \n",
       "sub-0923          53.0   20     38                    0             0   \n",
       "sub-0924          56.0   22     40                    0             0   \n",
       "sub-0925          44.0   13     28                    0             0   \n",
       "sub-0926          30.0   20     27                    1             1   \n",
       "sub-0927          38.0   23     35                    0             0   \n",
       "\n",
       "                hcp_mmp  schaefer_400  gordon  difumo_256  juelich  basc_444  \n",
       "participant_id                                                                \n",
       "sub-0001          522.0        1393.0  2264.0      3139.0   4020.0    4901.0  \n",
       "sub-0002          762.0        1633.0  2504.0      3381.0   4262.0    5143.0  \n",
       "sub-0003          263.0        1134.0  2005.0      2878.0   3759.0    4640.0  \n",
       "sub-0004          774.0        1645.0  2516.0      3393.0   4274.0    5155.0  \n",
       "sub-0005          291.0        1162.0  2033.0      2906.0   3787.0    4668.0  \n",
       "...                 ...           ...     ...         ...      ...       ...  \n",
       "sub-0923          344.0        1215.0  2086.0      2959.0   3840.0    4721.0  \n",
       "sub-0924          769.0        1640.0  2511.0      3388.0   4269.0    5150.0  \n",
       "sub-0925          416.0        1287.0  2158.0      3032.0   3913.0    4794.0  \n",
       "sub-0926          521.0        1392.0  2263.0      3138.0   4019.0    4900.0  \n",
       "sub-0927          435.0        1306.0  2177.0      3051.0   3932.0    4813.0  \n",
       "\n",
       "[871 rows x 16 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Useful directories\n",
    "main_dr = dirname(abspath(os.getcwd()))\n",
    "data_dr = os.path.join(main_dr, 'data')\n",
    "\n",
    "# This is optional, but speeds up some\n",
    "# operations, to ignore, set to None\n",
    "cache_loc = os.path.join(data_dr, 'cache', 'fmri')\n",
    "\n",
    "# Load in our pre-saved dataset\n",
    "data = bp.read_pickle(os.path.join(data_dr, 'datasets', 'fmri.dataset'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "numeric-meter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing test split on: 871 subjects.\n",
      "random_state: None\n",
      "Test split size: 0.2\n",
      "\n",
      "Performed train/test split\n",
      "Train size: 696\n",
      "Test size:  175\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"float: left; padding: 10px;\">\n",
       "        <h3>Data</h3><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basc_444</th>\n",
       "      <th>difumo_256</th>\n",
       "      <th>gordon</th>\n",
       "      <th>hcp_mmp</th>\n",
       "      <th>juelich</th>\n",
       "      <th>schaefer_400</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(249, 121, 93, .25)\">sub-0001</th>\n",
       "      <td>Loc(4901)</td>\n",
       "      <td>Loc(3139)</td>\n",
       "      <td>Loc(2264)</td>\n",
       "      <td>Loc(522)</td>\n",
       "      <td>Loc(4020)</td>\n",
       "      <td>Loc(1393)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0002</th>\n",
       "      <td>Loc(5143)</td>\n",
       "      <td>Loc(3381)</td>\n",
       "      <td>Loc(2504)</td>\n",
       "      <td>Loc(762)</td>\n",
       "      <td>Loc(4262)</td>\n",
       "      <td>Loc(1633)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0003</th>\n",
       "      <td>Loc(4640)</td>\n",
       "      <td>Loc(2878)</td>\n",
       "      <td>Loc(2005)</td>\n",
       "      <td>Loc(263)</td>\n",
       "      <td>Loc(3759)</td>\n",
       "      <td>Loc(1134)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0004</th>\n",
       "      <td>Loc(5155)</td>\n",
       "      <td>Loc(3393)</td>\n",
       "      <td>Loc(2516)</td>\n",
       "      <td>Loc(774)</td>\n",
       "      <td>Loc(4274)</td>\n",
       "      <td>Loc(1645)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(249, 121, 93, .25)\">sub-0005</th>\n",
       "      <td>Loc(4668)</td>\n",
       "      <td>Loc(2906)</td>\n",
       "      <td>Loc(2033)</td>\n",
       "      <td>Loc(291)</td>\n",
       "      <td>Loc(3787)</td>\n",
       "      <td>Loc(1162)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0923</th>\n",
       "      <td>Loc(4721)</td>\n",
       "      <td>Loc(2959)</td>\n",
       "      <td>Loc(2086)</td>\n",
       "      <td>Loc(344)</td>\n",
       "      <td>Loc(3840)</td>\n",
       "      <td>Loc(1215)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0924</th>\n",
       "      <td>Loc(5150)</td>\n",
       "      <td>Loc(3388)</td>\n",
       "      <td>Loc(2511)</td>\n",
       "      <td>Loc(769)</td>\n",
       "      <td>Loc(4269)</td>\n",
       "      <td>Loc(1640)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0925</th>\n",
       "      <td>Loc(4794)</td>\n",
       "      <td>Loc(3032)</td>\n",
       "      <td>Loc(2158)</td>\n",
       "      <td>Loc(416)</td>\n",
       "      <td>Loc(3913)</td>\n",
       "      <td>Loc(1287)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0926</th>\n",
       "      <td>Loc(4900)</td>\n",
       "      <td>Loc(3138)</td>\n",
       "      <td>Loc(2263)</td>\n",
       "      <td>Loc(521)</td>\n",
       "      <td>Loc(4019)</td>\n",
       "      <td>Loc(1392)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0927</th>\n",
       "      <td>Loc(4813)</td>\n",
       "      <td>Loc(3051)</td>\n",
       "      <td>Loc(2177)</td>\n",
       "      <td>Loc(435)</td>\n",
       "      <td>Loc(3932)</td>\n",
       "      <td>Loc(1306)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>871 rows × 6 columns</p>\n",
       "<p style=\"margin-top: .35em\"><span style=\"background: RGBA(176, 224, 230, .25)\">696 rows × 6 columns - Train Set </span></p><p style=\"margin-top: .35em\"><span style=\"background: RGBA(249, 121, 93, .25)\">175 rows × 6 columns - Test Set </span></p></div></div>\n",
       "<div style=\"float: left; padding: 10px;\">\n",
       "        <h3>Targets</h3><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIS</th>\n",
       "      <th>BMI</th>\n",
       "      <th>IST_intelligence_total</th>\n",
       "      <th>NEO_N</th>\n",
       "      <th>STAI_T</th>\n",
       "      <th>age</th>\n",
       "      <th>education_level</th>\n",
       "      <th>religious_now</th>\n",
       "      <th>religious_upbringing</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(249, 121, 93, .25)\">sub-0001</th>\n",
       "      <td>16</td>\n",
       "      <td>23.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>28</td>\n",
       "      <td>44.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0002</th>\n",
       "      <td>20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>30</td>\n",
       "      <td>31.0</td>\n",
       "      <td>21.75</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0003</th>\n",
       "      <td>20</td>\n",
       "      <td>31.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>34</td>\n",
       "      <td>40.0</td>\n",
       "      <td>25.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0004</th>\n",
       "      <td>22</td>\n",
       "      <td>20.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>29</td>\n",
       "      <td>32.0</td>\n",
       "      <td>22.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(249, 121, 93, .25)\">sub-0005</th>\n",
       "      <td>20</td>\n",
       "      <td>23.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>27</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0923</th>\n",
       "      <td>20</td>\n",
       "      <td>19.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>38</td>\n",
       "      <td>53.0</td>\n",
       "      <td>21.75</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0924</th>\n",
       "      <td>22</td>\n",
       "      <td>21.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>40</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0925</th>\n",
       "      <td>13</td>\n",
       "      <td>30.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>28</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0926</th>\n",
       "      <td>20</td>\n",
       "      <td>22.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>27</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th style=\"background: RGBA(176, 224, 230, .25)\">sub-0927</th>\n",
       "      <td>23</td>\n",
       "      <td>35.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>35</td>\n",
       "      <td>38.0</td>\n",
       "      <td>24.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>871 rows × 10 columns</p>\n",
       "<p style=\"margin-top: .35em\"><span style=\"background: RGBA(176, 224, 230, .25)\">696 rows × 10 columns - Train Set </span></p><p style=\"margin-top: .35em\"><span style=\"background: RGBA(249, 121, 93, .25)\">175 rows × 10 columns - Test Set </span></p></div></div>\n"
      ],
      "text/plain": [
       "                 BMI  IST_intelligence_total    age sex education_level  \\\n",
       "participant_id                                                            \n",
       "sub-0001        23.0                   159.0  22.00   0               2   \n",
       "sub-0002        20.0                   199.0  21.75   0               2   \n",
       "sub-0003        31.0                   227.0  25.25   0               0   \n",
       "sub-0004        20.0                   270.0  22.50   0               0   \n",
       "sub-0005        23.0                   212.0  22.25   1               0   \n",
       "...              ...                     ...    ...  ..             ...   \n",
       "sub-0923        19.0                   153.0  21.75   1               2   \n",
       "sub-0924        21.0                   246.0  22.25   1               2   \n",
       "sub-0925        30.0                   150.0  25.25   1               2   \n",
       "sub-0926        22.0                   161.0  20.75   1               0   \n",
       "sub-0927        35.0                   190.0  24.25   0               2   \n",
       "\n",
       "                STAI_T  BIS  NEO_N religious_upbringing religious_now  \\\n",
       "participant_id                                                          \n",
       "sub-0001          44.0   16     28                    0             1   \n",
       "sub-0002          31.0   20     30                    0             0   \n",
       "sub-0003          40.0   20     34                    0             0   \n",
       "sub-0004          32.0   22     29                    1             0   \n",
       "sub-0005          23.0   20     27                    0             0   \n",
       "...                ...  ...    ...                  ...           ...   \n",
       "sub-0923          53.0   20     38                    0             0   \n",
       "sub-0924          56.0   22     40                    0             0   \n",
       "sub-0925          44.0   13     28                    0             0   \n",
       "sub-0926          30.0   20     27                    1             1   \n",
       "sub-0927          38.0   23     35                    0             0   \n",
       "\n",
       "                hcp_mmp  schaefer_400  gordon  difumo_256  juelich  basc_444  \n",
       "participant_id                                                                \n",
       "sub-0001          522.0        1393.0  2264.0      3139.0   4020.0    4901.0  \n",
       "sub-0002          762.0        1633.0  2504.0      3381.0   4262.0    5143.0  \n",
       "sub-0003          263.0        1134.0  2005.0      2878.0   3759.0    4640.0  \n",
       "sub-0004          774.0        1645.0  2516.0      3393.0   4274.0    5155.0  \n",
       "sub-0005          291.0        1162.0  2033.0      2906.0   3787.0    4668.0  \n",
       "...                 ...           ...     ...         ...      ...       ...  \n",
       "sub-0923          344.0        1215.0  2086.0      2959.0   3840.0    4721.0  \n",
       "sub-0924          769.0        1640.0  2511.0      3388.0   4269.0    5150.0  \n",
       "sub-0925          416.0        1287.0  2158.0      3032.0   3913.0    4794.0  \n",
       "sub-0926          521.0        1392.0  2263.0      3138.0   4019.0    4900.0  \n",
       "sub-0927          435.0        1306.0  2177.0      3051.0   3932.0    4813.0  \n",
       "\n",
       "[871 rows x 16 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.set_test_split(.2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "joined-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectFpr, f_regression, f_classif\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import numpy as np\n",
    "import BPt as bp\n",
    "\n",
    "class CPMEdgeSum(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, sum_type='all'):\n",
    "        self.sum_type = sum_type\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        def sum_x(x):\n",
    "            return np.expand_dims(x.sum(axis=-1), axis=-1)\n",
    "        \n",
    "        if self.sum_type == 'all':\n",
    "            return sum_x(X)\n",
    "        \n",
    "        elif self.sum_type == 'pos':\n",
    "            X_temp = X.copy()\n",
    "            X_temp[X<0] = 0\n",
    "            return sum_x(X_temp)\n",
    "        \n",
    "        elif self.sum_type == 'neg':\n",
    "            X_temp = X.copy()\n",
    "            X_temp[X>0] = 0\n",
    "            return sum_x(X_temp)\n",
    "        \n",
    "        elif self.sum_type == 'both':\n",
    "            \n",
    "            X_temp = X.copy()\n",
    "            X_temp[X<0] = 0\n",
    "            pos = sum_x(X_temp)\n",
    "            \n",
    "            X_temp = X.copy()\n",
    "            X_temp[X>0] = 0\n",
    "            neg = sum_x(X_temp)\n",
    "\n",
    "            return np.hstack([pos, neg])\n",
    "\n",
    "def get_cpm_trans(sum_type):\n",
    "    return bp.Transformer(CPMEdgeSum(sum_type))\n",
    "\n",
    "\n",
    "def get_cpm_pipe(alpha=0.05, sum_type='pos',\n",
    "                 model_str='linear', is_binary=False):\n",
    "\n",
    "    # Make a custom feature selector, p-val selection at alpha\n",
    "    scoring = f_classif\n",
    "    if is_binary:\n",
    "        scoring = f_regression\n",
    "\n",
    "    fs = bp.FeatSelector(SelectFpr(scoring, alpha=alpha))\n",
    "    \n",
    "    # Get CPM style sum\n",
    "    cpm_trans = get_cpm_trans(sum_type)\n",
    "    \n",
    "    # Use base correlation connectivity\n",
    "    con = ConnectivityMeasure(kind='correlation',\n",
    "                              discard_diagonal=True,\n",
    "                              vectorize=True)\n",
    "    corr_loader = bp.Loader(con, behav='all', cache_loc=cache_loc)\n",
    "    \n",
    "    # Use a just linear model\n",
    "    model = bp.Model(model_str)\n",
    "    \n",
    "    # Return as pipeline\n",
    "    return bp.Pipeline([corr_loader, fs, cpm_trans, model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "comprehensive-sierra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8713be763a6f49b2ac139cb17e78d4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compare:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90bd368871d48de967764b4d2112701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folds:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean_scores_explained_variance</th>\n",
       "      <th>mean_scores_neg_mean_squared_error</th>\n",
       "      <th>std_scores_explained_variance</th>\n",
       "      <th>std_scores_neg_mean_squared_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>steps__1__obj__alpha</th>\n",
       "      <th>steps__2__obj__sum_type</th>\n",
       "      <th>steps__3__obj</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <th>pos</th>\n",
       "      <th>linear</th>\n",
       "      <td>-0.004325</td>\n",
       "      <td>-2.899389</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.981367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <th>pos</th>\n",
       "      <th>linear</th>\n",
       "      <td>-0.004721</td>\n",
       "      <td>-2.900978</td>\n",
       "      <td>0.006987</td>\n",
       "      <td>0.982426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <th>neg</th>\n",
       "      <th>linear</th>\n",
       "      <td>0.001763</td>\n",
       "      <td>-2.890226</td>\n",
       "      <td>0.045146</td>\n",
       "      <td>0.986089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <th>neg</th>\n",
       "      <th>linear</th>\n",
       "      <td>0.002721</td>\n",
       "      <td>-2.888502</td>\n",
       "      <td>0.048697</td>\n",
       "      <td>0.986772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <th>all</th>\n",
       "      <th>linear</th>\n",
       "      <td>-0.003993</td>\n",
       "      <td>-2.899643</td>\n",
       "      <td>0.014054</td>\n",
       "      <td>0.982777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <th>all</th>\n",
       "      <th>linear</th>\n",
       "      <td>-0.003851</td>\n",
       "      <td>-2.899923</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.984309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <th>both</th>\n",
       "      <th>linear</th>\n",
       "      <td>-0.000962</td>\n",
       "      <td>-2.899290</td>\n",
       "      <td>0.051597</td>\n",
       "      <td>0.986009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <th>both</th>\n",
       "      <th>linear</th>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-2.899655</td>\n",
       "      <td>0.053081</td>\n",
       "      <td>0.987766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <th>pos</th>\n",
       "      <th>svm</th>\n",
       "      <td>-0.049992</td>\n",
       "      <td>-2.997322</td>\n",
       "      <td>0.123344</td>\n",
       "      <td>1.050151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <th>pos</th>\n",
       "      <th>svm</th>\n",
       "      <td>-0.027824</td>\n",
       "      <td>-2.941842</td>\n",
       "      <td>0.110235</td>\n",
       "      <td>1.043352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <th>neg</th>\n",
       "      <th>svm</th>\n",
       "      <td>-0.022081</td>\n",
       "      <td>-2.948687</td>\n",
       "      <td>0.161920</td>\n",
       "      <td>1.045361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <th>neg</th>\n",
       "      <th>svm</th>\n",
       "      <td>-0.028953</td>\n",
       "      <td>-2.962768</td>\n",
       "      <td>0.146805</td>\n",
       "      <td>1.033013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <th>all</th>\n",
       "      <th>svm</th>\n",
       "      <td>-0.031195</td>\n",
       "      <td>-2.958438</td>\n",
       "      <td>0.137024</td>\n",
       "      <td>1.048536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <th>all</th>\n",
       "      <th>svm</th>\n",
       "      <td>-0.018098</td>\n",
       "      <td>-2.928718</td>\n",
       "      <td>0.132954</td>\n",
       "      <td>1.046333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <th>both</th>\n",
       "      <th>svm</th>\n",
       "      <td>-0.012169</td>\n",
       "      <td>-2.917853</td>\n",
       "      <td>0.107789</td>\n",
       "      <td>1.017463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <th>both</th>\n",
       "      <th>svm</th>\n",
       "      <td>-0.011626</td>\n",
       "      <td>-2.919841</td>\n",
       "      <td>0.101941</td>\n",
       "      <td>1.021428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            mean_scores_explained_variance  \\\n",
       "steps__1__obj__alpha steps__2__obj__sum_type steps__3__obj                                   \n",
       "0.05                 pos                     linear                              -0.004325   \n",
       "0.01                 pos                     linear                              -0.004721   \n",
       "0.05                 neg                     linear                               0.001763   \n",
       "0.01                 neg                     linear                               0.002721   \n",
       "0.05                 all                     linear                              -0.003993   \n",
       "0.01                 all                     linear                              -0.003851   \n",
       "0.05                 both                    linear                              -0.000962   \n",
       "0.01                 both                    linear                              -0.000603   \n",
       "0.05                 pos                     svm                                 -0.049992   \n",
       "0.01                 pos                     svm                                 -0.027824   \n",
       "0.05                 neg                     svm                                 -0.022081   \n",
       "0.01                 neg                     svm                                 -0.028953   \n",
       "0.05                 all                     svm                                 -0.031195   \n",
       "0.01                 all                     svm                                 -0.018098   \n",
       "0.05                 both                    svm                                 -0.012169   \n",
       "0.01                 both                    svm                                 -0.011626   \n",
       "\n",
       "                                                            mean_scores_neg_mean_squared_error  \\\n",
       "steps__1__obj__alpha steps__2__obj__sum_type steps__3__obj                                       \n",
       "0.05                 pos                     linear                                  -2.899389   \n",
       "0.01                 pos                     linear                                  -2.900978   \n",
       "0.05                 neg                     linear                                  -2.890226   \n",
       "0.01                 neg                     linear                                  -2.888502   \n",
       "0.05                 all                     linear                                  -2.899643   \n",
       "0.01                 all                     linear                                  -2.899923   \n",
       "0.05                 both                    linear                                  -2.899290   \n",
       "0.01                 both                    linear                                  -2.899655   \n",
       "0.05                 pos                     svm                                     -2.997322   \n",
       "0.01                 pos                     svm                                     -2.941842   \n",
       "0.05                 neg                     svm                                     -2.948687   \n",
       "0.01                 neg                     svm                                     -2.962768   \n",
       "0.05                 all                     svm                                     -2.958438   \n",
       "0.01                 all                     svm                                     -2.928718   \n",
       "0.05                 both                    svm                                     -2.917853   \n",
       "0.01                 both                    svm                                     -2.919841   \n",
       "\n",
       "                                                            std_scores_explained_variance  \\\n",
       "steps__1__obj__alpha steps__2__obj__sum_type steps__3__obj                                  \n",
       "0.05                 pos                     linear                              0.009576   \n",
       "0.01                 pos                     linear                              0.006987   \n",
       "0.05                 neg                     linear                              0.045146   \n",
       "0.01                 neg                     linear                              0.048697   \n",
       "0.05                 all                     linear                              0.014054   \n",
       "0.01                 all                     linear                              0.021985   \n",
       "0.05                 both                    linear                              0.051597   \n",
       "0.01                 both                    linear                              0.053081   \n",
       "0.05                 pos                     svm                                 0.123344   \n",
       "0.01                 pos                     svm                                 0.110235   \n",
       "0.05                 neg                     svm                                 0.161920   \n",
       "0.01                 neg                     svm                                 0.146805   \n",
       "0.05                 all                     svm                                 0.137024   \n",
       "0.01                 all                     svm                                 0.132954   \n",
       "0.05                 both                    svm                                 0.107789   \n",
       "0.01                 both                    svm                                 0.101941   \n",
       "\n",
       "                                                            std_scores_neg_mean_squared_error  \n",
       "steps__1__obj__alpha steps__2__obj__sum_type steps__3__obj                                     \n",
       "0.05                 pos                     linear                                  0.981367  \n",
       "0.01                 pos                     linear                                  0.982426  \n",
       "0.05                 neg                     linear                                  0.986089  \n",
       "0.01                 neg                     linear                                  0.986772  \n",
       "0.05                 all                     linear                                  0.982777  \n",
       "0.01                 all                     linear                                  0.984309  \n",
       "0.05                 both                    linear                                  0.986009  \n",
       "0.01                 both                    linear                                  0.987766  \n",
       "0.05                 pos                     svm                                     1.050151  \n",
       "0.01                 pos                     svm                                     1.043352  \n",
       "0.05                 neg                     svm                                     1.045361  \n",
       "0.01                 neg                     svm                                     1.033013  \n",
       "0.05                 all                     svm                                     1.048536  \n",
       "0.01                 all                     svm                                     1.046333  \n",
       "0.05                 both                    svm                                     1.017463  \n",
       "0.01                 both                    svm                                     1.021428  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = get_cpm_pipe(alpha=bp.Compare([.05, .01]),\n",
    "                    sum_type=bp.Compare(['pos', 'neg', 'all', 'both']),\n",
    "                    model_str=bp.Compare(['linear', 'svm']),\n",
    "                    is_binary=False)\n",
    "\n",
    "results = bp.evaluate(pipe, data,\n",
    "                      scope='basc_444', target='age',\n",
    "                      subjects='train', cv=5, \n",
    "                      eval_verbose=0)\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dominant-publication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0238ebfcbb42ef9d09782860d35c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BPtEvaluator\n",
       "------------\n",
       "explained_variance: 0.0303 ± 0.0190\n",
       "neg_mean_squared_error: -2.80 ± 0.2915\n",
       "\n",
       "Saved Attributes: ['estimators', 'preds', 'timing', 'train_subjects', 'val_subjects', 'feat_names', 'ps', 'mean_scores', 'std_scores', 'weighted_mean_scores', 'scores', 'fis_', 'coef_', 'cv']\n",
       "\n",
       "Avaliable Methods: ['to_pickle', 'compare', 'get_X_transform_df', 'get_inverse_fis', 'get_preds_dfs', 'subset_by', 'get_fis', 'get_coef_', 'permutation_importance']\n",
       "\n",
       "Evaluated With:\n",
       "target: age\n",
       "problem_type: regression\n",
       "scope: basc_444\n",
       "subjects: train\n",
       "random_state: 1\n"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from BPt.default.pipelines import ridge_pipe\n",
    "\n",
    "con = ConnectivityMeasure(kind='correlation',\n",
    "                          discard_diagonal=True,\n",
    "                          vectorize=True)\n",
    "corr_loader = bp.Loader(con, behav='all', cache_loc=cache_loc)\n",
    "\n",
    "pipe = bp.Pipeline([corr_loader, ridge_pipe])\n",
    "\n",
    "results = bp.evaluate(pipe, data,\n",
    "                      scope='basc_444', target='age',\n",
    "                      subjects='train', cv=5, \n",
    "                      eval_verbose=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "acquired-consideration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcea39ca49a4be9a88201747531c59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BPtEvaluator\n",
       "------------\n",
       "explained_variance: 0.0759 ± 0.0105\n",
       "neg_mean_squared_error: -2.67 ± 0.3123\n",
       "\n",
       "Saved Attributes: ['estimators', 'preds', 'timing', 'train_subjects', 'val_subjects', 'feat_names', 'ps', 'mean_scores', 'std_scores', 'weighted_mean_scores', 'scores', 'fis_', 'coef_', 'cv']\n",
       "\n",
       "Avaliable Methods: ['to_pickle', 'compare', 'get_X_transform_df', 'get_inverse_fis', 'get_preds_dfs', 'subset_by', 'get_fis', 'get_coef_', 'permutation_importance']\n",
       "\n",
       "Evaluated With:\n",
       "target: age\n",
       "problem_type: regression\n",
       "scope: basc_444\n",
       "subjects: train\n",
       "random_state: 1\n"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = ConnectivityMeasure(kind='tangent',\n",
    "                          discard_diagonal=True,\n",
    "                          vectorize=True)\n",
    "loader = bp.Loader(con, behav='all', cache_loc=cache_loc)\n",
    "\n",
    "pipe = bp.Pipeline([loader, ridge_pipe])\n",
    "\n",
    "results = bp.evaluate(pipe, data,\n",
    "                      scope='basc_444', target='age',\n",
    "                      subjects='train', cv=5, \n",
    "                      eval_verbose=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-marketplace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "compare_target = bp.Compare(data.get_cols('target'))\n",
    "compare_scope = bp.Compare(['difumo_256', 'gordon'])\n",
    "\n",
    "\n",
    "\n",
    "ps = bp.ProblemSpec(target=compare_target, \n",
    "                    scope=compare_scope,\n",
    "                    n_jobs=6,\n",
    "                    random_state=1\n",
    "                   )\n",
    "\n",
    "# This is the base object responsible for generating the correlation matrix\n",
    "con = ConnectivityMeasure(kind='tangent',\n",
    "                          discard_diagonal=True,\n",
    "                          vectorize=True)\n",
    "\n",
    "# We just need to wrap it in a BPt Loader object\n",
    "loader = bp.Loader(con, behav='all', cache_loc=cache_loc)\n",
    "\n",
    "# The base model we will try is ridge regression\n",
    "# with a random search for hyper-parameters over different params\n",
    "ridge_rs = bp.Model('ridge', params=1,\n",
    "                    param_search=bp.ParamSearch(n_iter=30))\n",
    "elastic_rs = bp.Model('elastic', params=1,\n",
    "                      param_search=bp.ParamSearch(n_iter=60))\n",
    "svm_rs = bp.Model('svm', params=1,\n",
    "                  param_search=bp.ParamSearch(n_iter=60))\n",
    "\n",
    "# Wrap in compare\n",
    "compare_model = bp.Compare([bp.Option(ridge_rs, 'ridge')\n",
    "                            bp.Option(elastic_rs, 'elastic'),\n",
    "                            bp.Option(svm_rs, 'svm')])\n",
    "\n",
    "# Put the steps together in a pipeline\n",
    "pipe = bp.Pipeline([loader, compare_model], cache_loc=cache_loc)\n",
    "\n",
    "# Evaluate\n",
    "results = bp.evaluate(pipe, data, ps, mute_warnings=True)\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-backup",
   "metadata": {},
   "source": [
    "To begin, we will fix the target variable of interest as BMI, and evaluate using just the gordon parcellation. These parameters we can specify in the parameter holding ProblemSpec class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = bp.ProblemSpec(target='BMI', \n",
    "                    scope='gordon', # This defines the subset of columns to use\n",
    "                    n_jobs=6,\n",
    "                    random_state=1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-jesus",
   "metadata": {},
   "source": [
    "Notably, our raw data is still in the form of time-series which by default are not in a format that most ML models can accept. The first step of our pipeline will therefore be to transform these ROI timeseries in some way. The first method we will try is the most basic / common in which we generate a correlation matrix between each ROI's timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "# This is the base object responsible for generating the correlation matrix\n",
    "con = ConnectivityMeasure(kind='correlation',\n",
    "                          discard_diagonal=True,\n",
    "                          vectorize=True)\n",
    "\n",
    "# We just need to wrap it in a BPt Loader object\n",
    "loader = bp.Loader(con, behav='all', cache_loc=None)\n",
    "\n",
    "# The base model we will try is ridge regression\n",
    "# with a random search for hyper-parameters over different params\n",
    "ridge_rs = bp.Model('ridge', params=1,\n",
    "                    param_search=bp.ParamSearch(n_iter=10))\n",
    "\n",
    "# Put the steps together in a pipeline\n",
    "pipe = bp.Pipeline([loader, ridge_rs], cache_loc=cache_loc)\n",
    "\n",
    "# Evaluate\n",
    "results = bp.evaluate(pipe, data, ps, mute_warnings=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-toronto",
   "metadata": {},
   "source": [
    "The next variation we will try is employing tangent connectivity instead of the base correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same steps as before, but this time using kind='tangent'\n",
    "con = ConnectivityMeasure(kind='tangent',\n",
    "                          discard_diagonal=True,\n",
    "                          vectorize=True)\n",
    "loader = bp.Loader(con, behav='all', cache_loc=cache_loc)\n",
    "\n",
    "# Replace model in pipeline\n",
    "pipe = bp.Pipeline([loader, ridge_rs], cache_loc=cache_loc)\n",
    "\n",
    "# Evaluate\n",
    "results = bp.evaluate(pipe, data, ps, mute_warnings=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-caribbean",
   "metadata": {},
   "source": [
    "Not bad, just by changing the \"kind\" of connectivity, we make a big jump in performance.\n",
    "\n",
    "What if we want to try a different parcellation? Well all we have to do is change the scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are our options\n",
    "parcs = list(data['data'])\n",
    "\n",
    "parcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-intervention",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Change the scope to be a compare object\n",
    "ps.scope = bp.Compare(parcs)\n",
    "\n",
    "# Now re-run evaluate, but this time there is an extra compare\n",
    "# loop which will run the same 5-fold evaluation, but for all\n",
    "# parcellations seperately.\n",
    "results = bp.evaluate(pipe, data, ps, mute_warnings=True)\n",
    "results.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpt",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
